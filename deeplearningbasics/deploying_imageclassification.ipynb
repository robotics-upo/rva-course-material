{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deploying-imageclassification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM992kCu2QLFpt90pydWn/S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robotics-upo/rva-course-material/blob/master/deeplearningbasics/deploying_imageclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjS9FyyX2NNc"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this session, we will see how to use a model already trained in our code with OpenCV.\n",
        "\n",
        "We will consider the model we trained in the former session, and also how to use models from TensorFlow Hub, both as is and for transfer learning.\n",
        "\n",
        "* **TensorFlow**: open source library for machine learning https://www.tensorflow.org/\n",
        "* **Keras** high-level API for TensorFlow\n",
        "* **TensorFlow Hub**: repository of trained models https://www.tensorflow.org/hub?hl=es-419\n",
        "* **OpenCV**: http://opencv.org\n",
        "\n",
        "We will use it to detect objects from the CIFAR categories in images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPwzc8rJE5dk"
      },
      "source": [
        "#Numpy module\n",
        "import numpy as np\n",
        "\n",
        "#Import OpenCV\n",
        "import cv2\n",
        "#Import tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "#We can use OpenCV in Colab, but not its functions for creating plots\n",
        "#We use matplotlib for generating plots\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "#We use the library scikit to read images from url \n",
        "#In OpenCV, the function to read from file is cv2.imread\n",
        "from skimage import io\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDzuvSKWFSCw"
      },
      "source": [
        "# Loading data and models\n",
        "\n",
        "First, we will load the model we trained in the previous session from file. We can mount in the Colab machine Google Drive and load the file from there.\n",
        "\n",
        "You can load the models and files from your local folders if you are not using Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvnSxf6Z4YqI"
      },
      "source": [
        "#Mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cspwphInG3Rb"
      },
      "source": [
        "I am assuming you have a folder called **colabfiles** in your Google Drive **root** folder. \n",
        "\n",
        "Within the folder, we have the model **classif.h5**.\n",
        "\n",
        "If the folders and files are called differently, change the paths and names.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZLavo8LG099"
      },
      "source": [
        "#Let's load the CNN model using the Keras API\n",
        "\n",
        "model = tf.keras.models.load_model('/content/drive/My Drive/colabfiles/classif.h5')\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV4MnPsCQaCh"
      },
      "source": [
        "Once we have finished with Google Drive we can unmount it. If you need it for further tasks, leave it mounted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ozZbkj0QhdB"
      },
      "source": [
        "#Unmount the drive if we are not using it more\n",
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKIta01dKnnv"
      },
      "source": [
        "Let's load an example image and show it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2WYF4_DHAJY"
      },
      "source": [
        "#Let's load an image\n",
        "imrgb = io.imread('https://robotics.upo.es/~lmercab/rva/test.jpg')\n",
        "\n",
        "plt.imshow(imrgb, cmap=plt.cm.binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f4eryXIK673"
      },
      "source": [
        "# Using our CNN for prediction\n",
        "\n",
        "We can process patches through our network. Recall our network receives as inputs 32x32 images (normalized between 0 and 1) and outputs probabilities for each of the 10 classes of CIFAR-10.\n",
        "\n",
        "\n",
        "In the next example, we select a 32x32 patch corresponding to one of the cars.\n",
        "\n",
        "Notice that the method `predict` expects a batch of images (that is, for this network a tensor of (N, 32, 32,3), where N is the size of the batch. That is why we augment the dimension of the patch. This can be done in NumPy in several ways:\n",
        "\n",
        "* Using `expand_dims`: https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html\n",
        "* Using NumPy indexing and slicing and the `newaxis` object: https://numpy.org/doc/stable/reference/arrays.indexing.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcIIGVbuK91v"
      },
      "source": [
        "cifar10_labels = ['airplane', 'automobile', 'bird','cat','deer','dog','frog',\n",
        "'horse','ship', 'truck']\n",
        "\n",
        "#32x32 image patch, normalized\n",
        "im_patch = imrgb[140:140+32,75:75+32]/255.0\n",
        "plt.imshow(im_patch, cmap=plt.cm.binary)\n",
        "\n",
        "#The network expect as inputs sets of images. We have to expand the dimension of\n",
        "#the image\n",
        "prediction = model.predict(im_patch[np.newaxis,...])\n",
        "\n",
        "#The former operation is equivalent to the following two:\n",
        "#im_patch_final = np.expand_dims(im_patch, 0)\n",
        "#prediction = model.predict(im_patch_final)\n",
        "\n",
        "print('Network output', prediction)\n",
        "print(\"PredicciÃ³n del modelo: \", cifar10_labels[np.argmax(prediction[0])] )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjiQ4qlzavqh"
      },
      "source": [
        "We can use our trained network to look for cars in the image. Also, we want to detect multiple cars, not only one.\n",
        "\n",
        "The idea is to cover the whole image, extracting patches of 32x32 and taking those ones in which the network predicts as automobile (class 1 in the CIFAR-10 dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RHNDHPCRsmq"
      },
      "source": [
        "#We make a copy to draw on it\n",
        "im_result = imrgb.copy()\n",
        "\n",
        "prob_threshold = 0.7\n",
        "\n",
        "#Loop over the image looking for cars\n",
        "#We extract 32x32 patches each time. We go in steps of\n",
        "#32. This could be reduced to cover densely the image, at the cost of time\n",
        "for r in range(0,imrgb.shape[0] - 32, 32):\n",
        "  for c in range(0,imrgb.shape[1] - 32, 32):\n",
        "    im_patch = imrgb[r:r+32,c:c+32]/255.0\n",
        "    prediction = model.predict(im_patch[np.newaxis,...])\n",
        "    #Draw a rectangle on the original image if the probability of car\n",
        "    #(class 1) is over a threshold\n",
        "    if(np.argmax(prediction[0]) == 1 and prediction[0][1]>prob_threshold):\n",
        "      upper_left = (c, r)\n",
        "      bottom_right = (c + 32, r+32)\n",
        "      #Draw a red rectangle\n",
        "      cv2.rectangle(im_result,upper_left, bottom_right, (255,0,0), 2)\n",
        "     \n",
        "plt.imshow(im_result, cmap=plt.cm.binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3Ct1f_6bknl"
      },
      "source": [
        "# Searching at different scales\n",
        "\n",
        "The main problem is that our network expects cars as 32x32 patches. Of course, in our image there are cars larger than that. \n",
        "\n",
        "We need to search for cars in multiple scales in the image. For that, we can use image pyramids as we have seen in former sessions\n",
        "\n",
        "* https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_pyramids/py_pyramids.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6wmUffwV_ij"
      },
      "source": [
        "#Create an image pyramid\n",
        "#Dowsample image by 2\n",
        "imrgb_2 = cv2.pyrDown(imrgb)\n",
        "plt.imshow(imrgb_2, cmap=plt.cm.binary)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZhoG7ACWqIy"
      },
      "source": [
        "#Downsample the former by 2 (so the original by 4)\n",
        "imrgb_4 = cv2.pyrDown(imrgb_2)\n",
        "plt.imshow(imrgb_4, cmap=plt.cm.binary)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWA1C3VHW6ed"
      },
      "source": [
        "#Search in downsampled images\n",
        "\n",
        "#im_result_4 = imrgb_4.copy()\n",
        "for r in range(0,imrgb_4.shape[0] - 32, 8):\n",
        "  for c in range(0,imrgb_4.shape[1] - 32, 8):\n",
        "    im_patch = imrgb_4[r:r+32,c:c+32]/255.0\n",
        "    prediction = model.predict(im_patch[np.newaxis,...])\n",
        "    #Draw a rectangle on the original image if the probability of car\n",
        "    #(class 1) is over a threshold\n",
        "    if(np.argmax(prediction[0]) == 1 and prediction[0][1]>prob_threshold):\n",
        "      #Draw in the original scale image\n",
        "      upper_left = (c*4, r*4)\n",
        "      bottom_right = (c*4 + 32*4, r*4+32*4)\n",
        "      cv2.rectangle(im_result,upper_left, bottom_right, (0,255,0), 2)\n",
        "\n",
        "for r in range(0,imrgb_2.shape[0] - 32, 16):\n",
        "  for c in range(0,imrgb_2.shape[1] - 32, 16):\n",
        "    im_patch = imrgb_2[r:r+32,c:c+32]/255.0\n",
        "    prediction = model.predict(im_patch[np.newaxis,...])\n",
        "    #Draw a rectangle on the original image if the probability of car\n",
        "    #(class 1) is over a threshold\n",
        "    if(np.argmax(prediction[0]) == 1 and prediction[0][1]>prob_threshold):\n",
        "      #Draw in the original scale image\n",
        "      upper_left = (c*2, r*2)\n",
        "      bottom_right = (c*2 + 32*2, r*2+32*2)\n",
        "      cv2.rectangle(im_result,upper_left, bottom_right, (0,0,255), 2)\n",
        "     \n",
        "plt.imshow(im_result, cmap=plt.cm.binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EgAXKjhsjrE"
      },
      "source": [
        "As you can see, the process of searching over the image for the objects is costly the way we do it. \n",
        "\n",
        "Actually, one can design **object detection networks** desgined to output the bounding boxes and classes for the objects directly from the image efficiently, reusing a lot of operations. Recall that CNNs intrinsically also search at different scales, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nI8JTiv3hLo"
      },
      "source": [
        "#Using pre-trained models\n",
        "\n",
        "Actually, TensorFlow Hub (and frameworks have similar things) contains many pre-trained models that you can use in your code.\n",
        "\n",
        "For instance, the Mobilenetv2 model for image classification, trained in the ImageNet dataset\n",
        "\n",
        "* TF Hub Model: https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\n",
        "* ImageNet: https://www.image-net.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GXkFUOh3l1s"
      },
      "source": [
        "#Utilities to use TensorFlow Hub\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "classifier_model =\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\"\n",
        "\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "\n",
        "#We create a sequential network using Keras with \"just one layer\", which is the\n",
        "#Mobilenetv2 model itself \n",
        "classifier = tf.keras.Sequential([\n",
        "    hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE+(3,))\n",
        "])\n",
        "\n",
        "#We download here the labels for the ImageNet classes. Imagenet considers\n",
        "#1001 different classes\n",
        "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "imagenet_labels = np.array(open(labels_path).read().splitlines())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7JbtUonuQGj"
      },
      "source": [
        "Let's try some examples. The network requires as inputs images of size (224,224). You can resize an image using `tf.image.resize`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOcUrMiM39So"
      },
      "source": [
        "im_input = io.imread('https://robotics.upo.es/~lmercab/rva/coach.jpg')\n",
        "\n",
        "im_input = im_input/255.0\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(im_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A7ik0Wmu1Ow"
      },
      "source": [
        "Using the method `predict` we can obtain the vector of probabilities. With `np.argmax`, the class with highest probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScRpTyv64Nbj"
      },
      "source": [
        "print('Original size:', im_input.shape)\n",
        "res = tf.image.resize(im_input, IMAGE_SHAPE)\n",
        "\n",
        "print('New size:', res.shape)\n",
        "\n",
        "#The network returns a 1001-size vector of probabilities for the different classes\n",
        "result = classifier.predict(res[np.newaxis, ...])\n",
        "print('Output shape',result.shape)\n",
        "\n",
        "predicted_class = np.argmax(result[0], axis=-1)\n",
        "print('Predicted class',predicted_class)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbr2qa53vBWb"
      },
      "source": [
        "Recall that in `imagenet_labels` we have the textual labels for the different classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMRFuDHV4sHf"
      },
      "source": [
        "plt.figure()  \n",
        "plt.imshow(res)\n",
        "plt.axis('off')\n",
        "predicted_class_name = imagenet_labels[predicted_class]\n",
        "_ = plt.title(\"Prediction: \" + predicted_class_name.title())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjoHikIavUeY"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "One very interesting applications of pre-trained models is transfer-learning. That is, using a model trained in a given dataset to apply to a related task. \n",
        "\n",
        "The idea behind is that if the features that the network has learned to extract are general enough, they should also work in the new related task.\n",
        "\n",
        "In this case, we can just train the last \"classification layers\" and keep the weights of the convolutional layers intact.\n",
        "\n",
        "We are goint to use this to address the CIFAR-10 classification problem using the Mobilenetv2 model trained in the much larger ImageNet dataset.   \n",
        "\n",
        "First, let's download the CIFAR-10 dataset as we did in the last session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdbXGPXD7lGs"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#Load train dataset for CIFAR10\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "plt.imshow(x_test[100], cmap=plt.cm.binary)\n",
        "\n",
        "#Normalize data\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htL0EdxyvZPX"
      },
      "source": [
        "For transfer learning, instead of getting the whole Mobilenetv2 model, we obtain all its layers except the classification part. \n",
        "\n",
        "TensorFlow Hub has available this at: https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\n",
        "\n",
        "Thus, we create a sequential model, as we have done before. We use the \"feature network\" from Mobilenetv2 as the first layer (remember, this internally consists of many layers). But now we add to the output of the features a dense layer with sigmoid activation to obtain the 10 classes of CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtU31Emv70YF"
      },
      "source": [
        "#Create the Keras model\n",
        "#The feature network provides 1280-dimensional \"flattened\" vectors.\n",
        "#We add after that a fully connected layer to obtain the output\n",
        "transfer_model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", output_shape=[1280],\n",
        "                   trainable=False),  # Can be True.\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "transfer_model.build([None, 224, 224, 3])  # Batch input shape.\n",
        "\n",
        "print(transfer_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2BvWjY-xzwg"
      },
      "source": [
        "Now, we can train the former network with the CIFAR-10 training set. \n",
        "\n",
        "Pay attention to the parameter `trainable=False` for the Mobilenetv2 feature network. This means that, when training, we actually will be adjusting the weights of the last classification layer, and not the rest. So we have a very big network, but thanks to transfer learning we will reuse many weight values.\n",
        "\n",
        "Alterantively you can set `trainable` to `True`. In this case, you can also fine tune the weights of that part.\n",
        "\n",
        "Before training, we should resize the images to the expected input size of the Mobilenetv2 network. We will only use the first 10000 images of CIFAR due to memory constraints of the Colab machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXXacEeA8qsK"
      },
      "source": [
        "x_resized = tf.image.resize(x_train[0:10000,:,:,:], IMAGE_SHAPE)\n",
        "\n",
        "print(x_resized.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNlQz0c3yn_1"
      },
      "source": [
        "Let's train the new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aETNCGm28MSE"
      },
      "source": [
        "transfer_model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "_ = transfer_model.fit(x_resized, y_train[0:10000,:], batch_size=32, validation_split=0.1, epochs=10, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjttVHth0iZC"
      },
      "source": [
        "We can see the accuracy of the new model on the test set (again, we just use 1000 images of the 10000 of the test set due to RAM restrictions. A better handling of memory in our code can solve this)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvm9TOi28OT_"
      },
      "source": [
        "x_resized = tf.image.resize(x_test[2000:3000,:,:,:], IMAGE_SHAPE)\n",
        "\n",
        "_ , test_acc = transfer_model.evaluate(x_resized, y_test[2000:3000,:])\n",
        "\n",
        "print('Accuracy:', test_acc*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XkevhBJ1ba3"
      },
      "source": [
        "Let's use it to see the prediction on the same image as above. We select the same patch. Compare the values of the probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzZpWRaP07O7"
      },
      "source": [
        "#32x32 image patch, normalized\n",
        "im_patch = imrgb[140:140+32,75:75+32]/255.0\n",
        "\n",
        "#Resize the patch to the expected input size\n",
        "im_patch = tf.image.resize(im_patch, IMAGE_SHAPE)\n",
        "\n",
        "plt.imshow(im_patch, cmap=plt.cm.binary)\n",
        "\n",
        "#The network expect as inputs sets of images. We have to expand the dimension of\n",
        "#the image\n",
        "prediction = transfer_model.predict(im_patch[np.newaxis,...])\n",
        "\n",
        "print('Network output:', prediction)\n",
        "print('Model prediction:', cifar10_labels[np.argmax(prediction[0])] )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}